{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß¨ Training pipeline: Training retrieval model\n",
    "\n",
    "In this notebook, you will train a retrieval model that will be able to quickly generate a small subset of candidate items from a large collection of items. Your model will be based on the *two-tower architecture*, which embeds queries and candidates (keys) into a shared low-dimensional vector space. Here, a query consists of features of a customer and a transaction (e.g. timestamp of the purchase), whereas a candidate consists of features of a particular item. All queries will have a user ID and all candidates will have an item ID, and the model will be trained such that the embedding of a user will be close to all the embeddings of items the user has previously bought.\n",
    "\n",
    "After training the model you will save and upload its components to the Vertex AI Model Registry.\n",
    "\n",
    "Let's go ahead and load the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from loguru import logger\n",
    "\n",
    "from recsys import gcp_integrations, training\n",
    "from recsys.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Connect to Vertex AI Feature Online Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-11 15:04:56.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mrecsys.gcp_integrations.feature_store\u001b[0m:\u001b[36mget_feature_store\u001b[0m:\u001b[36m17\u001b[0m - \u001b[1mRetrieving Feature Store from us-central1/recsys-dev-gonzo/recsys_feature_store_dev\u001b[0m\n",
      "\u001b[32m2025-02-11 15:04:57.154\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mrecsys.gcp_integrations.feature_store\u001b[0m:\u001b[36mget_feature_store\u001b[0m:\u001b[36m37\u001b[0m - \u001b[31m\u001b[1mError retrieving Feature Store: 'FeatureOnlineStoreServiceClient' object has no attribute 'feature_online_store_path'\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'FeatureOnlineStoreServiceClient' object has no attribute 'feature_online_store_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m project, fs \u001b[38;5;241m=\u001b[39m \u001b[43mgcp_integrations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_store\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Github/GenAI_Custom_Real_Time_Personalized_Recommender/recsys/gcp_integrations/feature_store.py:28\u001b[0m, in \u001b[0;36mget_feature_store\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     client \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mgapic\u001b[38;5;241m.\u001b[39mFeatureOnlineStoreServiceClient()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Format the feature store path\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     feature_store_path \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_online_store_path\u001b[49m(\n\u001b[1;32m     29\u001b[0m         project\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mGCP_PROJECT,\n\u001b[1;32m     30\u001b[0m         location\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mGCP_LOCATION,\n\u001b[1;32m     31\u001b[0m         feature_online_store\u001b[38;5;241m=\u001b[39msettings\u001b[38;5;241m.\u001b[39mVERTEX_FEATURE_STORE_ID,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feature_store_path, client\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FeatureOnlineStoreServiceClient' object has no attribute 'feature_online_store_path'"
     ]
    }
   ],
   "source": [
    "project, fs = gcp_integrations.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíø Create training dataset\n",
    "You will train your retrieval model with a subset of features.\n",
    "\n",
    "For the query embedding you will use:\n",
    "- `customer_id`: ID of the customer.\n",
    "- `age`: age of the customer at the time of purchase.\n",
    "- `month_sin`, `month_cos`: time of year the purchase was made.\n",
    "\n",
    "For the candidate embedding you will use:\n",
    "- `article_id`: ID of the item.\n",
    "- `garment_group_name`: type of garment.\n",
    "- `index_group_name`: menswear/ladieswear etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_view = gcp_integrations.feature_store.create_retrieval_feature_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = training.two_tower.TwoTowerDataset(\n",
    "    feature_view=feature_view, batch_size=settings.TWO_TOWER_MODEL_BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_ds, val_ds = dataset.get_train_val_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Training samples: {len(dataset.properties['train_df']):,}\")\n",
    "logger.info(f\"Validation samples: {len(dataset.properties['val_df']):,}\")\n",
    "\n",
    "logger.info(f\"Number of users: {len(dataset.properties['user_ids']):,}\")\n",
    "logger.info(f\"Number of items: {len(dataset.properties['item_ids']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.properties[\"train_df\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóºüóº Build the Two Tower model\n",
    "\n",
    "The two tower model consist of two models:\n",
    "- **Query model**: Generates a query representation of a given user and transaction features.\n",
    "- **Candidate model**: Generates an item representation given item features.\n",
    "\n",
    "**Both models produce embeddings that live in the same embedding space**. You let this space be low-dimensional to prevent overfitting on the training data. (Otherwise, the model might simply memorize previous purchases, which makes it recommend items customers already have bought).\n",
    "\n",
    "You start with creating the query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_model_factory = training.two_tower.QueryTowerFactory(dataset=dataset)\n",
    "query_model = query_model_factory.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will evaluate the two tower model using the *top-100 accuracy*. That is, for each transaction in the validation data you will generate the associated query embedding and retrieve the set of the 100 items that are closest to this query in the embedding space. The top-100 accuracy measures how often the item that was actually bought is part of this subset. To evaluate this, you create a dataset of all unique items in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = training.two_tower.TwoTowerTrainer(dataset=dataset, model=model)\n",
    "history = trainer.train(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "# Training loss subplot\n",
    "ax1.plot(history.history[\"loss\"], label=\"Training Loss\", color=\"blue\")\n",
    "ax1.set_title(\"Training Loss Over Time\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Validation loss subplot\n",
    "ax2.plot(history.history[\"val_loss\"], label=\"Validation Loss\", color=\"red\")\n",
    "ax2.set_title(\"Validation Loss Over Time\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()  # Uncomment to show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Upload models to Vertex AI model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO: CREATE MODEL REGISTRY IN TERRAFORM AND FUNCTIONS TO RETRIEVE IT\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO - DO\n",
    "# ALSO CREATE THE METHODS TO UPLOAD THE MODEL TO THE REGISTRY\n",
    "\n",
    "None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
