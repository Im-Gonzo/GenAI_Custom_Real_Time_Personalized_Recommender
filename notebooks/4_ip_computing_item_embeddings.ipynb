{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üë©üèª‚Äçüî¨ Offline inference pipeline: Computing item embeddings\n",
    "\n",
    "In this notebook you will compute the candidate embeddings and populate a Hopsworks feature group with a vector index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "import polars as pl\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from loguru import logger\n",
    "from recsys.config import settings\n",
    "from recsys.gcp.bigquery import client as bq_client\n",
    "from recsys.core.embeddings.computation import compute_embeddings\n",
    "from recsys.gcp.feature_store.datasets import create_training_dataset\n",
    "from recsys.core.embeddings.preprocessing import preprocess_candidates\n",
    "from recsys.data.preprocessing.splitting import train_validation_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()[:-9]\n",
    "fullpath = os.path.join(path, 'data/preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = pl.read_csv(f'{fullpath}/transactions.csv')\n",
    "articles_df = pl.read_parquet(f'{fullpath}/articles.parquet')\n",
    "customers_df = pl.read_csv(f'{fullpath}/customers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing candidate embeddings\n",
    "\n",
    "You start by computing candidate embeddings for all items in the training data.\n",
    "\n",
    "First, you load your candidate model. Recall that you uploaded it to the Vertex AI Model Registry in previous steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(path, 'notebooks/ranking_model/ranking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_model = joblib.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get candidates data\n",
    "\n",
    "Now, we get the training retrieval data containing all the features required for the candidate embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = create_training_dataset(trans_df, articles_df, customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_def, _, _, _ = train_validation_test_split(\n",
    "    df=training_data,\n",
    "    validation_size=settings.TWO_TOWER_DATASET_VALIDATION_SPLIT_SIZE,\n",
    "    test_size=settings.TWO_TOWER_DATASET_TEST_SPLIT_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings\n",
    "\n",
    "Next you compute the embeddings of all candidate items that were used to train the retrieval model.\n",
    "\n",
    "We can recover this features from the X_train.columns from the previous notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['age',\n",
    " 'product_type_name',\n",
    " 'product_group_name',\n",
    " 'graphical_appearance_name',\n",
    " 'colour_group_name',\n",
    " 'perceived_colour_value_name',\n",
    " 'perceived_colour_master_name',\n",
    " 'department_name',\n",
    " 'index_name',\n",
    " 'index_group_name',\n",
    " 'section_name',\n",
    " 'garment_group_name',\n",
    " 'month_sin',\n",
    " 'month_cos',\n",
    " 'article_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_df = preprocess_candidates(train_df, features)\n",
    "item_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df = compute_embeddings(item_df, candidate_model)\n",
    "embeddings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#ff5f27\">Create Vertex AI Embedding Index </span>\n",
    "\n",
    "Now you are ready to create a feature group for your candidate embeddings.\n",
    "\n",
    "To begin with, you need to create your Embedding Index where you will specify the name of the embeddings feature and the embeddings length.\n",
    "Then you attach this index to the FV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Uploading 'candidates' Feature to BigQuery.\")\n",
    "bq_client.load_features(candidates_df=embeddings_df)\n",
    "logger.info(\"‚úÖ Uploaded 'candidates' Feature to BigQuery!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
